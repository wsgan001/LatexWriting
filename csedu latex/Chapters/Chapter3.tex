% Chapter 3

\chapter{Proposed Approach} % Write in your own chapter title
\label{Chapter 3}
\lhead{Chapter 3. \emph{Proposed Approach}} % Write in your own chapter title to set the page header
In this chapter we present our proposed approach for finding infrequent itemset mining over data stream. We discuss about the definition, process and algorithms of our proposed approach. 
%\section{Introduction}
%
%\section{Terminologies}
%
%\section{Problem Definition and Solution Methodology}
%
\section{Problem Definition}
%
This paper address the problem of infrequent weighted itemset mining from a transaction dataset. Let {\it I} = \{{\it $i_1$, $i_2$,. . . . . ,$i_m$}\} a set of data items. Transactional dataset {\it T} =\{{\it $t_1$, $t_2$, $t_3$,. . . . . ,$t_n$}\} is a set of transaction where each transaction is a set of items in {\it I} and denoted by transaction ID (tid). 

An Item set is a set of data items i.e. in {\it k} item sets there is {\it k} items.  Support for an item is the count of that item in the dataset. Based on support count, an item may be frequent or infrequent. For identify an item as frequent or infrequent, a threshold {\it E} is used. This threshold can be set based on user interest. In this paper, we are targeting infrequent items. So, we need to discard items that are above the threshold. 

In regular basis, for frequent item set mining, items and transactions are consider in the same way. But \cite{cagliero2014infrequent} according to this paper, we also treating each item differently. Here, each item is paired with it’s weight i.e ({\it I,k}), {\it I} is an item contain in the transaction set {\it T}, {\it K} is the weight of the item that signifies the interest/intensity in the transaction.

As we are working on data stream, we need to process data in an efficient manner. To find infrequent itemset mining from a data stream, we can’t perform multiple scan from a data stream. Once the streams flow  through we lose them. To find recent important knowledge from a data stream, Single-pass and sliding window based mechanism \cite{ahmed2008shrfp} is required. 

We have adopted similar definitions presented in the previous works (Carter er al., 1997; Barbar and Hamilton, 2000, 2001, 2003; Li et al., 2005a,b). Let {\it I} = \{{\it $i_1$, $i_2$, . . . . . . , $i_m$}\} be a set of items and {\it D} be a transaction database \{{\it $T_1$, $T_2$,. . . . . . , $T_n$}\} where each transaction {\it Ti} ϵ {\it D} is a subset of {\it I}.\\
\textbf{Definition 1:} The measure value mv(ip, Tq), represents the weight of an item ip in transaction Tq. For example, in Table~\ref{tab:Table}, mv(a,T3) = 43. \\
\textbf{Definition 2:} The transaction measure value of a transaction Tq denoted as tmv(Tq) means the total measure value of a transaction Tq and it is defined by, 
\begin{equation}
tmv (Tq) = \sum_{ip \in  Tq} mv(ip,Tq)
\end{equation}
\\ For example, tmv(T4) = mv(a, T4) + mv(c, T4)+ mv(d, T4) = 100 +43+100 = 243 in Table~\ref{tab:Table}.


\textbf{Definition 3:} The total measure value Tmv(DB) represents the total measure value in DB. It is defined by,
\begin{equation}
Tmv (DB) = \sum_{Tq \in DB}\sum_{ip \in  Tq} mv(ip,Tq)
\end{equation}
\\For example, Tmv(DB) = 1140 in Table~\ref{tab:Table} for window 2. \\

\textbf{Definition 4:} The itemset measure value of an itemset X in transaction Tq, imv(X, Tq) is defined by,
\begin{equation}
imv (X, Tq) = \sum_{ip \in X} mv(ip,Tq)
\end{equation}
\\where X = \{{\it $i_1$, $i_2$, . . . . . . , $i_k$}\} is a k-itemset,  % \{ X \subseteq Tq\} and 1\leq k\leq m$. For example, imv(bc, T2) = 3 + 2 = 5 in Table~\ref{tab:Table}. \\
\\
\textbf{Definition 5:} The local measure value of an itemset X is defined by, 
\begin{equation}
lmv (X) = \sum_{Tq \in DBX}\sum_{ip \in X} mv(ip,Tq)
\end{equation}
\\where DBX is the set of transactions contain itemset X. For example, lmv(bc) = imv(bc, T5) +imv(bc, T5) = 71 + 57 + 32 + 11= 171 in Table~\ref{tab:Table} for window 2. \\

\textbf{Definition 6:} The share value of an itemset X, denoted as SH(X), is the ratio of the local measure value of X to the total measure value in DB. So, SH(X) is defined by,
\begin{equation}
SH(X) = \frac{lmv(X)}{Tmv(DB)}
\end{equation}
\\For example, SH(bc) = 171/1140 = 0.15, in Table~\ref{tab:Table} in window 2.\\
\textbf{Definition 8:} Given a maximum share threshold, $maxShare$, an itemset is share-infrequent if $SH(X) \ge maxShare$. If maxShare is 0.25 (we can also express it as 25\%), in the example database, “bc” is a share-infrequent itemset, as SH(bc) = 0.15. \\

\section{Solution Methodology}
%
In this section, we describe our proposed approach for mining infrequent weighted itemset. Our approach is divided in two portions. In first portion, we construct the tree with weighted inputs. 
In second portion, we pass the tree for mining process.\\
Construction process of our tree structure is to store the stream data using a single pass. We use ShrFP-Tree (Share-frequent pattern tree) for share-infrequent pattern mining. The header table 
is maintained to keep an item order in our tree structure. Each entry of a transaction in a header table  explicitly maintains item-id and weight information for each item. Here, 
weight is the {\it tmv} (transaction measure value) values of items. To facilitate the tree traversals adjacent links are also maintained (not shown in figure for simplicity) in our tree structure.
%
\par Consider the example data stream at Table~\ref{tab:Table} . At first, ShrFP-Tree captures the {\it tmv} values of all items and keep in header table  as weight. After that, 
we scan each database transaction one by one and then insert in the tree. In our example, first transaction T1 contains four items one of which is 0. If any item’s weight is 0 in the transaction, 
we will not insert that item in the tree. So, now we have three transactions {\it b,c,d} and insert them in the tree.\\
\begin{figure}[ht]

{\centering
\begin{minipage}{.4\textwidth}
  \centering
  
	\begin{center}
	\begin{tabular}{ |c|c| } 
	\hline
	Item & Weight \\
	\hline
	b & 371\\
	c & 371\\
	d & 371\\
	\hline
	\end{tabular}
\end{center}  
\end{minipage}
\hfill
\begin{minipage}{0.60\textwidth}
  \centering
  \includegraphics[width=.9\textwidth]{01.eps}
\end{minipage}
}
\caption{Insertion of Batch 1}
\label{fig:insert1}
\end{figure} 
\\
Figure ~\ref{fig:insert1} shows the tree and header table after inserting batch 1.  For batch 2 insertion in the tree, we calculate tmv for transactions. Then check in any item contains 0. 
If any item having 0, then omit it from the transaction. After that insert the transaction in the tree.
\begin{figure}[ht]

{\centering
\begin{minipage}{.4\textwidth}
  \centering
  
	\begin{center}
	\begin{tabular}{ |c|c| } 
	\hline
	Item & Weight \\
	\hline
	a & 372\\
	b & 371\\
	c & 743\\
	d & 743\\
	\hline
	\end{tabular}
\end{center}  
\end{minipage}
\hfill
\begin{minipage}{0.60\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{02.eps}
\end{minipage}
}
\caption{Insertion of Batch 2}
\label{fig:insert2}
\end{figure} 
\\
Figure ~\ref{fig:insert2} shows the tree and header table after inserting batch 2. In the same way, batch 3 also inserted in the tree. The final tree after inserting window 1 in the tree 
is shown in Figure ~\ref{fig:insert3}.
\begin{figure}[ht]

{\centering
\begin{minipage}{.4\textwidth}
  \centering
  
	\begin{center}
	\begin{tabular}{ |c|c| } 
	\hline
	Item & Weight \\
	\hline
	a & 856\\
	b & 855\\
	c & 1028\\
	d & 1227\\
	\hline
	\end{tabular}
\end{center}  
\end{minipage}
\hfill
\begin{minipage}{0.60\textwidth}
  \centering
  \includegraphics[width=1\textwidth]{03.eps}
\end{minipage}
}
\caption{Insertion of Batch 3}
\label{fig:insert3}
\end{figure} 
\\
\par As it’s a data stream, the stream moves to batch 4. It needs to delete previous information of batch 1. Because batch 1 is not belongs to window 2 
and therefore the information of batch 1 becomes garbage in the window 2 and it will no longer be used. We delete information of batch 1 from the tree. We also update the header table. 
After deletion of batch 1, the header table and the tree is shown in Figure ~\ref{fig:insert5}. Some nodes which do not have any information of batch 2 and batch 3. 
We can delete them from the tree. In case of other nodes, the weight are shifted one position left to remove the weight information of batch 1.
%
\begin{figure}[ht]

{\centering
\begin{minipage}{.4\textwidth}
  \centering
  
	\begin{center}
	\begin{tabular}{ |c|c| } 
	\hline
	Item & Weight \\
	\hline
	a & 856\\
	b & 484\\
	c & 657\\
	d & 856\\
	\hline
	\end{tabular}
\end{center}  
\end{minipage}
\hfill
\begin{minipage}{0.60\textwidth}
  \centering
  \includegraphics[width=1.2\textwidth]{04.eps}
\end{minipage}
}
\caption{Deletion of Batch 1}
\label{fig:insert4}
\end{figure} 
\\

\begin{figure}[ht]
\centering
\includegraphics[scale = 0.9] {05.eps}
\caption{After deletion of Batch 1}
\label{fig:insert5}
\end{figure}

Now insert new batch i.e. batch 4 in the tree. As a result, the three weight information of each node represents batch 2, batch 3 and batch 4. Figure ~\ref{fig:insert6} shows the 
tree after insertion of batch 4.
\begin{figure}[ht]

{\centering
\begin{minipage}{.4\textwidth}
  \centering
  
	\begin{center}
	\begin{tabular}{ |c|c| } 
	\hline
	Item & Weight \\
	\hline
	a & 1140\\
	b & 768\\
	c & 791\\
	d & 1006\\
	\hline
	\end{tabular}
\end{center}  
\end{minipage}
\hfill
\begin{minipage}{0.60\textwidth}
  \centering
  \includegraphics[width=1.2\textwidth]{06.eps}
\end{minipage}
}
\caption{Insertion of Batch 4}
\label{fig:insert6}
\end{figure} 
\\

\begin{figure}[ht]
\centering
\includegraphics[scale = 0.8] {07.eps}
\caption{Complete tree after insertion of Batch 4}
\label{fig:insert7}
\end{figure}



 
%

{\bf Property 1:} The total value of {\it tmv} (transaction measure value) of any node in ShrFP-Tree is greater than or equal to the sum of total value of {\it tmv} values of its children.
\clearpage
\section{Mining Process}
ShrFP-Tree is a pattern growth mining process that mines all the candidate share-infrequent patterns. According to our property 1, pattern growth mining algorithm can be directly applicable to it by using {\it tmv} values.
\par Consider the database of Table~\ref{tab:Table} and maxShare = 0.20 in that database. The final ShrFP-Tree for the database we have considered in Table~\ref{tab:Table}, is shown in Figure ~\ref{fig:insert7}. At first, we have prepared a header table for all items. According to header table, least weighted item will be mined first. In our example, {\it b} is the first item for mining. A conditional tree is prepared for item {\it b} is shown in ~\ref{fig:insert8}. For preparing conditional tree, we have taken all the branches prefixing the item {\it b}. Based on max{\_}lmv value item {\it b} is not in candidate infrequent pattern list. But its candidate pattern can be in infrequent list. So, we have added them in the candidate infrequent pattern list. Then again, we have started for item {\it c}. Same process is applied for generating conditional tree for item {\it c} shown in ~\ref{fig:insert9}. For item d, generation conditional tree is shown in ~\ref{fig:insert10} Then we have completed the process for generating candidate infrequent pattern list. Table 2 shows the calculation process for finding actual share-infrequent patterns from the candidate infrequent pattern list. The resultant share-infrequent patterns are \{{\it c}\},\{{\it b,c}\},\{{\it b,c,d}\} for window 2.



\begin{figure}[ht]
\centering
\includegraphics[scale = 0.8] {10.eps}
\caption{Conditional tree for item b}
\label{fig:insert8}
\end{figure}


\begin{figure}[ht]

{\centering
\begin{minipage}{.4\textwidth}
  \centering
  	\begin{center}
	\begin{tabular}{ |c|c| } 
	\hline
	Item & Weight \\
	\hline
	a & 791\\
	b & 419\\
	\hline
	\end{tabular}
\end{center}  
\end{minipage}
\hfill
\begin{minipage}{0.60\textwidth}
  \centering
  \includegraphics[width=0.4\textwidth]{09.eps}
\end{minipage}
}
\caption{Conditional tree for item c}
\label{fig:insert9}
\end{figure} 


\begin{figure}[ht]

{\centering
\begin{minipage}{.4\textwidth}
  \centering
  
	\begin{center}
	\begin{tabular}{ |c|c| } 
	\hline
	Item & Weight \\
	\hline
	a & 1006\\
	b & 634\\
	c & 657\\
	\hline
	\end{tabular}
\end{center}  
\end{minipage}
\hfill
\begin{minipage}{0.60\textwidth}
  \centering
  \includegraphics[width=0.8\textwidth]{08.eps}
\end{minipage}
}
\caption{Conditional tree for item d}
\label{fig:insert10}
\end{figure} 



\begin{table}
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
\hline
Patterns & lmv & SH & Shared Infrequent Patterns \\
\hline
{\it b} & 235 & 0.206 & No\\
{\it c} & 154 & 0.135 & Yes\\
{\it d} & 357 & 0.313 & No\\
{\it ab} & 486 & 0.426 & No\\
{\it bc} & 171 & 0.152 & Yes\\
{\it ac} & 474 & 0.415 & No\\
{\it cd} & 357 & 0.313 & No\\
{\it bd} & 417 & 0.365 & No\\
{\it abc} & 384 & 0.336 & No\\
{\it acd} & 372 & 0.326 & No\\
{\it abd} & 577 & 0.506 & No\\
{\it bcd} & 199 & 0.174 & Yes\\
{\it abcd} & 285 & 0.251 & No\\


\hline
\end{tabular}
\caption{Calculation process of Share-Infrequent patterns}
\label{tab:Table2}
\end{center}
\end{table}

\clearpage
\section{Proposed Algorithm}
Algorithm will be placed here.

%
%
%\section{Complexity Analysis}
%
%\section{Application of the Proposed Algorithm}
%
%\section{Example Workout}
%

